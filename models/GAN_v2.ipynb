{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\aisys\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.transforms import Resize, Compose, ToTensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, subset='train'):\n",
    "        \"\"\"\n",
    "        root_dir: 데이터셋의 최상위 디렉터리 경로\n",
    "        transform: 이미지에 적용할 전처리\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.subset = subset\n",
    "        self.filenames = os.listdir(os.path.join(root_dir, \"original\"))  # 원본 이미지 폴더를 기준으로 파일 이름 리스트 생성\n",
    "\n",
    "        if subset == 'train':\n",
    "            self.filenames = self.filenames[:5000]\n",
    "        elif subset == 'test':\n",
    "            self.filenames = self.filenames[5000:5100]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.filenames[idx]\n",
    "        original_path = os.path.join(self.root_dir, \"original\", img_name)  # 원본 이미지 경로\n",
    "        masked_path = os.path.join(self.root_dir, \"overlay\", img_name)      # 마스킹된 이미지 경로\n",
    "        mask_path = os.path.join(self.root_dir, \"mask\", img_name)          # 마스크 이미지 경로\n",
    "\n",
    "        original_img = Image.open(original_path).convert('RGB')\n",
    "        masked_img = Image.open(masked_path).convert('RGB')\n",
    "        mask_img = Image.open(mask_path).convert('L')\n",
    "\n",
    "        resize = Resize((256, 256))\n",
    "        mask_img = resize(mask_img)\n",
    "        \n",
    "        mask_img = np.array(mask_img)\n",
    "        mask_img = (mask_img > 128).astype(np.float32)  # 흰색은 1, 검은색은 0\n",
    "        mask_img = torch.from_numpy(mask_img).unsqueeze(0)\n",
    "\n",
    "        if self.transform:\n",
    "            original_img = self.transform(original_img)\n",
    "            masked_img = self.transform(masked_img)\n",
    "\n",
    "        return masked_img, mask_img, original_img\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "root_dir = 'C:/Users/Serin Kim/workspace/AISYS/data'\n",
    "train_dataset = CustomDataset(root_dir=root_dir, transform=transform, subset='train')\n",
    "test_dataset = CustomDataset(root_dir=root_dir, transform=transform, subset='test')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked images dimensions: torch.Size([32, 3, 256, 256])\n",
      "Masks dimensions: torch.Size([32, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "def check_dimensions(dataloader):\n",
    "    # 데이터 로더에서 첫 번째 배치를 가져옵니다.\n",
    "    masked_imgs, masks, _ = next(iter(dataloader))\n",
    "    \n",
    "    # 마스킹된 이미지와 마스크의 차원을 출력합니다.\n",
    "    print(\"Masked images dimensions:\", masked_imgs.shape)\n",
    "    print(\"Masks dimensions:\", masks.shape)\n",
    "\n",
    "# train_dataloader를 사용하여 차원을 확인합니다.\n",
    "check_dimensions(train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(4, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            # 추가적인 레이어\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "            # 추가적인 레이어\n",
    "        )\n",
    "\n",
    "    def forward(self, masked_img, mask):\n",
    "        x = torch.cat([masked_img, mask], dim=1)  # 마스킹된 이미지와 마스크를 채널 차원에서 결합\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# 판별기 정의\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 128 * 128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.main(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(generator, discriminator, criterion, g_optimizer, d_optimizer, dataloader, device, num_epochs, interval):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    saved_images = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (masked_imgs, masks, original_imgs) in enumerate(dataloader):\n",
    "            # 장치 할당\n",
    "            masked_imgs = masked_imgs.to(device)\n",
    "            masks = masks.to(device)\n",
    "            original_imgs = original_imgs.to(device)\n",
    "            real_labels = torch.ones(original_imgs.size(0), 1).to(device)\n",
    "            fake_labels = torch.zeros(original_imgs.size(0), 1).to(device)\n",
    "\n",
    "            # 진짜 이미지로 디스크리미네이터 학습\n",
    "            outputs = discriminator(original_imgs)\n",
    "            d_loss_real = criterion(outputs, real_labels)\n",
    "            real_score = outputs\n",
    "\n",
    "            # 가짜 이미지 생성 및 디스크리미네이터 학습\n",
    "            fake_images = generator(masked_imgs, masks)\n",
    "            outputs = discriminator(fake_images.detach())\n",
    "            d_loss_fake = criterion(outputs, fake_labels)\n",
    "            fake_score = outputs\n",
    "\n",
    "            # 디스크리미네이터 손실 계산 및 역전파\n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "            discriminator.zero_grad()\n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "\n",
    "            # 생성기 학습\n",
    "            outputs = discriminator(fake_images)\n",
    "            g_loss = criterion(outputs, real_labels)\n",
    "            \n",
    "            generator.zero_grad()\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "            \n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], '\n",
    "                      f'D Loss: {d_loss.item()}, G Loss: {g_loss.item()}, '\n",
    "                      f'Real Score: {real_score.mean().item()}, Fake Score: {fake_score.mean().item()}')\n",
    "\n",
    "        if (epoch + 1) % interval == 0:\n",
    "            with torch.no_grad():\n",
    "                saved_images.append(generator(masked_imgs, masks).detach().cpu())\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=len(saved_images), figsize=(15, 5))\n",
    "    for img, ax in zip(saved_images, axes):\n",
    "        ax.imshow(img[0].permute(1, 2, 0))  # 첫 번째 이미지의 [C, H, W]를 [H, W, C]로 변경\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def test(generator, dataloader, device):\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (masked_imgs, masks, _) in enumerate(dataloader):\n",
    "            masked_imgs = masked_imgs.to(device)\n",
    "            masks = masks.to(device)\n",
    "            fake_images = generator(masked_imgs, masks)\n",
    "            # 여기서 가짜 이미지를 평가하거나 저장할 수 있습니다.\n",
    "            if i == 0:  # 예시로 한 배치의 결과만 저장\n",
    "                torchvision.utils.save_image(fake_images, 'test_samples.png', normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델, 옵티마이저, 손실 함수 초기화\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 학습 및 테스트 실행\n",
    "train(generator, discriminator, criterion, g_optimizer, d_optimizer, train_dataloader, device, num_epochs=50, interval = 5)\n",
    "test(generator, test_dataloader, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DMFN Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\aisys\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Serin Kim\\workspace\\AISYS\\DMFN-master\\models\\inpainting_model.py:260: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if load_path_G is not '':\n",
      "c:\\Users\\Serin Kim\\workspace\\AISYS\\DMFN-master\\models\\inpainting_model.py:264: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if load_path_D is not '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(center_weight=1, dis_feature_criterion='l1', dis_feature_weight=5, display_num=8, f='\"c:\\\\Users\\\\Serin Kim\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-v2-82123qJ19yl6ROW3.json\"', feature_criterion='l1', feature_weight=25, fineSize=256, gan_type='vanilla', gan_weight=0.003, gpu_ids=[], img_shape=[3, 256, 256], in_nc=4, in_nc_D=3, is_train=True, log_iter=50, lr_D=0.0002, lr_G=0.0002, lr_gamma=0.5, lr_policy='MultiStepLR', lr_steps=[100000], n_res=8, nf=64, out_nc=3, pixel_criterion='l1', pixel_weight=1, pretrained_model_D='', pretrained_model_G='outputs/celeba-hq/checkpoints/58000_G.pth', save_image_iter=500, save_model_iter=2000, train=True, val_iter=200, which_model_D='discriminator', which_model_G='inpainting_resnet')\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Serin Kim\\workspace\\AISYS\\DMFN-master\\models\\inpainting_model.py:260: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if load_path_G is not '':\n",
      "c:\\Users\\Serin Kim\\workspace\\AISYS\\DMFN-master\\models\\inpainting_model.py:264: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if load_path_D is not '':\n",
      "c:\\Users\\Serin Kim\\workspace\\AISYS\\DMFN-master\\models\\inpainting_model.py:260: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if load_path_G is not '':\n",
      "c:\\Users\\Serin Kim\\workspace\\AISYS\\DMFN-master\\models\\inpainting_model.py:264: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if load_path_D is not '':\n",
      "c:\\Users\\Serin Kim\\workspace\\AISYS\\DMFN-master\\models\\inpainting_model.py:260: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if load_path_G is not '':\n",
      "c:\\Users\\Serin Kim\\workspace\\AISYS\\DMFN-master\\models\\inpainting_model.py:264: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if load_path_D is not '':\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Unsupported generator model: inpainting_resnet",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 76\u001b[0m\n\u001b[0;32m     74\u001b[0m args_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mvars\u001b[39m(args)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(args_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 76\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mInpaintingModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Serin Kim\\workspace\\AISYS\\DMFN-master\\models\\inpainting_model.py:17\u001b[0m, in \u001b[0;36mInpaintingModel.__init__\u001b[1;34m(self, opt)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_train \u001b[38;5;241m=\u001b[39m opt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# define networks and load pretrained model\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetG \u001b[38;5;241m=\u001b[39m \u001b[43mnetworks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefine_G\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_train:\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetD \u001b[38;5;241m=\u001b[39m networks\u001b[38;5;241m.\u001b[39mdefine_D(opt)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\Serin Kim\\workspace\\AISYS\\DMFN-master\\models\\networks.py:22\u001b[0m, in \u001b[0;36mdefine_G\u001b[1;34m(opt)\u001b[0m\n\u001b[0;32m     19\u001b[0m     netG \u001b[38;5;241m=\u001b[39m arch\u001b[38;5;241m.\u001b[39mInpaintingGenerator(in_nc\u001b[38;5;241m=\u001b[39mopt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124min_nc\u001b[39m\u001b[38;5;124m'\u001b[39m], out_nc\u001b[38;5;241m=\u001b[39mopt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout_nc\u001b[39m\u001b[38;5;124m'\u001b[39m], nf\u001b[38;5;241m=\u001b[39mopt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnf\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     20\u001b[0m                                     n_res\u001b[38;5;241m=\u001b[39mopt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_res\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnsupported generator model: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(which_model))\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_train\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     24\u001b[0m     netG\u001b[38;5;241m.\u001b[39mapply(weights_init)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Unsupported generator model: inpainting_resnet"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#DMFN\n",
    "from models import *\n",
    "import argparse\n",
    "import os\n",
    "from utils import get_config, prepare_sub_folder, _write_images, write_html\n",
    "from data import create_dataset, create_dataloader\n",
    "import math\n",
    "from models.inpainting_model import InpaintingModel\n",
    "#from tensorboardX import SummaryWriter\n",
    "import torch\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description=\"Setup training configurations\")\n",
    "\n",
    "    # 기본 설정\n",
    "    parser.add_argument('--fineSize', type=int, default=256, help='image dimensions')\n",
    "    parser.add_argument('--img_shape', type=int, nargs=3, default=[3, 256, 256], help='[channel, height, width]')\n",
    "    parser.add_argument('--is_train', type=bool, default=True, help='training mode')\n",
    "    parser.add_argument('--gpu_ids', type=int, nargs='*', default=[], help='GPU IDs to use')\n",
    "    parser.add_argument('--pretrained_model_G', type=str, default='outputs/celeba-hq/checkpoints/58000_G.pth', help='path to generator pretrained model')\n",
    "    parser.add_argument('--pretrained_model_D', type=str, default='', help='path to discriminator pretrained model')\n",
    "    parser.add_argument('--val_iter', type=int, default=200, help='validation interval')\n",
    "    parser.add_argument('--log_iter', type=int, default=50, help='logging interval')\n",
    "    parser.add_argument('--save_image_iter', type=int, default=500, help='save image interval')\n",
    "    parser.add_argument('--save_model_iter', type=int, default=2000, help='save model interval')\n",
    "    parser.add_argument('--display_num', type=int, default=8, help='number of images to display')\n",
    "    parser.add_argument('--f',type=str, default=r\"c:\\Users\\Serin Kim\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-8212WMzxoKBEaBMj.json\")\n",
    "\n",
    "    # 생성기 네트워크 설정\n",
    "    parser.add_argument('--which_model_G', type=str, default='inpainting_resnet', help='type of generator model')\n",
    "    parser.add_argument('--in_nc', type=int, default=4, help='input channel number for G')\n",
    "    parser.add_argument('--out_nc', type=int, default=3, help='output channel number for G')\n",
    "    parser.add_argument('--nf', type=int, default=64, help='number of filters for G and D')\n",
    "    parser.add_argument('--n_res', type=int, default=8, help='number of residual blocks in G')\n",
    "\n",
    "    # 판별기 네트워크 설정\n",
    "    parser.add_argument('--which_model_D', type=str, default='discriminator', help='type of discriminator model')\n",
    "    parser.add_argument('--in_nc_D', type=int, default=3, help='input channel number for D')\n",
    "\n",
    "    # 훈련 옵션\n",
    "\n",
    "    parser.add_argument('--train',type=bool, default=True)\n",
    "    # Adding arguments specifically related to training\n",
    "    parser.add_argument('--pixel_weight', type=float, default=1, help='Weight for pixel loss')\n",
    "    parser.add_argument('--pixel_criterion', type=str, default='l1', choices=['l1', 'ml1'], help='Criterion for pixel loss')\n",
    "    parser.add_argument('--feature_weight', type=float, default=25, help='Weight for feature loss')\n",
    "    parser.add_argument('--feature_criterion', type=str, default='l1', help='Criterion for feature loss')\n",
    "    parser.add_argument('--center_weight', type=float, default=1, help='Weight for center loss')\n",
    "    parser.add_argument('--dis_feature_weight', type=float, default=5, help='Weight for discriminator feature matching loss')\n",
    "    parser.add_argument('--dis_feature_criterion', type=str, default='l1', help='Criterion for discriminator feature matching loss')\n",
    "    parser.add_argument('--gan_weight', type=float, default=0.003, help='Weight for GAN loss')\n",
    "    parser.add_argument('--gan_type', type=str, default='vanilla', choices=['vanilla', 'lsgan'], help='Type of GAN loss')\n",
    "    parser.add_argument('--lr_G', type=float, default=0.0002, help='Learning rate for G')\n",
    "    parser.add_argument('--lr_D', type=float, default=0.0002, help='Learning rate for D')\n",
    "    parser.add_argument('--lr_policy', type=str, default='MultiStepLR', help='Learning rate policy')\n",
    "    parser.add_argument('--lr_steps', type=int, nargs='*', default=[100000], help='Steps at which the learning rate is decayed')\n",
    "    parser.add_argument('--lr_gamma', type=float, default=0.5, help='Gamma rate for learning rate decay')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "    #config = get_config(args.config)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # tensorboard\n",
    "    #model_name = os.path.splitext(os.path.basename(args))[0].split('_')[0]\n",
    "    #train_writer = SummaryWriter(os.path.join(args.output_path + '/logs', model_name))\n",
    "\n",
    "    #output_dir = os.path.join(args.output_path + '/outputs', model_name)\n",
    "    #    checkpoint_dir, image_dir = prepare_sub_folder(output_dir)\n",
    "    #args['checkpoint_dir'] = checkpoint_dir\n",
    "\n",
    "    print(args)\n",
    "    args_dict = vars(args)\n",
    "    print(args_dict['train'])\n",
    "    model = InpaintingModel(args_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Namespace' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnet_G\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Namespace' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "args['net_G']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aisys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
